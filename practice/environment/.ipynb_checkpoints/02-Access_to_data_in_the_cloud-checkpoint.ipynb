{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access to data in the cloud (GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gcsfs\n",
    "import xarray as xr\n",
    "import intake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from Google Cloud Storage (gcsfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access and listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cloud file system access point:\n",
    "fs = gcsfs.GCSFileSystem(project='argo-france', token='anon', access='read_only')\n",
    "\n",
    "# And list content of a bucket:\n",
    "fs.ls('argodata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But data access with ``gcsfs`` is critically dependant on the GCS set-up. For instance the following project does not allow to list the bucket content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs2 = gcsfs.GCSFileSystem(project='alert-ground-261008', token='anon', access='read_only')\n",
    "try:\n",
    "    fs2.ls('data_bdo2020')\n",
    "except:\n",
    "    print(sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, some dataset may not be free and use a requester pay model. \n",
    "In this case, you would have to properly manage authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs3 = gcsfs.GCSFileSystem(project='poised-honor-358', token='anon')\n",
    "try:\n",
    "    fs3.ls('somovar-02')\n",
    "except ValueError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsmap = fs.get_mapper(\"argodata/gridded/ISAS15_TEMP_NATL.zarr\")\n",
    "ds = xr.open_zarr(gcsmap)\n",
    "print(\"Size of the dataset:\", ds.nbytes/1e9,\"Gb\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load another dataset:\n",
    "# gcsmap = fs.get_mapper('argodata/sdl/GLOBAL_ARGO_SDL2000')\n",
    "gcsmap = fs.get_mapper('argodata/sdl/GLOB_HOMOGENEOUS_variables.zarr')\n",
    "ds = xr.open_zarr(gcsmap)\n",
    "print(\"Size of the dataset:\", ds.nbytes/1e9,\"Gb\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use intake catalog of data\n",
    "\n",
    "The catalog also uses the gcsfs entry point, but with intake it's transparent to the user:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access and listing of the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/obidam/ds2-2020/master/ds2_data_catalog.yml'\n",
    "cat = intake.Catalog(catalog_url)\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cat.en4.read_chunked()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pangeo data\n",
    "\n",
    "[Note that Pangeo is currently re-organising its datastore](https://discourse.pangeo.io/t/cleaning-out-the-pangeo-data-google-cloud-storage-bucket/353/22).\n",
    "\n",
    "Some dataset access points may thus be obsolete below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct access to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cloud file system access point:\n",
    "fs = gcsfs.GCSFileSystem(project='pangeo-181919', token='anon', access='read_only')\n",
    "fs.ls('pangeo-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a dataset:\n",
    "# gcsmap = gcsfs.mapping.GCSMap('pangeo-data/dataset-duacs-rep-global-merged-allsat-phy-l4-v3-alt', gcs=fs) # AVISO altimetry\n",
    "# gcsmap = gcsfs.mapping.GCSMap('pangeo-data/eNATL60-BLBT02X-ssh', gcs=fs) # High Resolution North Atlantic Ocean simulation\n",
    "gcsmap = gcsfs.mapping.GCSMap('pangeo-era5/reanalysis/spatial-analysis', gcs=fs) # Hourly Atmospheric fields (1979/2018)\n",
    "ds = xr.open_zarr(gcsmap)\n",
    "print(\"Size of the dataset:\", ds.nbytes/1e9,\"Gb\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intake Pangeo catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/master.yaml'\n",
    "cat = intake.Catalog(catalog_url)\n",
    "print(list(cat))\n",
    "print(list(cat.ocean))\n",
    "print(list(cat.atmosphere))\n",
    "print(list(cat.hydro))\n",
    "# cat.walk(depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = cat.ocean.SOSE.read_chunked()\n",
    "ds = cat.atmosphere.gmet_v1.read_chunked()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot a map\n",
    "%matplotlib inline\n",
    "%time sla = ds['adt'].sel(time='2009-02-12', method='nearest')\n",
    "print(sla)\n",
    "sla.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot a time series\n",
    "# (this slicing is much longer !)\n",
    "%time sla = ds['adt'].sel(latitude=30, method='nearest').sel(longitude=360-55, method='nearest')\n",
    "print(sla)\n",
    "sla.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obidam36",
   "language": "python",
   "name": "obidam36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
